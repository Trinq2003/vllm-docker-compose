# =================================================================
# Deployment Repository - Environment Configuration
# =================================================================
#
# Copy this file to .env and fill in your actual values:
# cp env.example .env
#
# Required variables are marked with [REQUIRED]
# Optional variables have default values indicated
# =================================================================

# =================================================================
# HUGGING FACE CONFIGURATION [REQUIRED]
# =================================================================
# Get your token from: https://huggingface.co/settings/tokens
# Required for downloading models from Hugging Face
HUGGING_FACE_HUB_TOKEN=your_huggingface_token_here

# =================================================================
# DATABASE CONFIGURATION
# =================================================================

# PostgreSQL for LiteLLM
POSTGRES_DB=litellm
POSTGRES_USER=llmproxy
POSTGRES_PASSWORD=your_secure_postgres_password_here
POSTGRES_HOST=db
POSTGRES_PORT=5432

# MySQL for RAGFlow
MYSQL_ROOT_PASSWORD=your_secure_mysql_root_password_here
MYSQL_DATABASE=rag_flow
MYSQL_USER=ragflow_user
MYSQL_PASSWORD=your_secure_mysql_password_here
MYSQL_HOST=mysql
MYSQL_PORT=3306

# Redis Configuration
REDIS_PASSWORD=your_secure_redis_password_here
REDIS_HOST=redis
REDIS_PORT=6379

# =================================================================
# MINIO OBJECT STORAGE CONFIGURATION
# =================================================================
MINIO_ROOT_USER=your_minio_username
MINIO_ROOT_PASSWORD=your_secure_minio_password_here
MINIO_CONSOLE_PORT=9001
MINIO_PORT=9000
MINIO_HOST=minio
MINIO_BUCKET=ragflow-bucket

# =================================================================
# ELASTICSEARCH CONFIGURATION
# =================================================================
# Elasticsearch for RAGFlow vector storage
ELASTIC_PASSWORD=your_secure_elastic_password_here
STACK_VERSION=8.11.3
ES_PORT=1200
ES_HOST=elasticsearch

# Kibana (optional, for Elasticsearch management)
KIBANA_PORT=6601
KIBANA_USER=rag_flow
KIBANA_PASSWORD=infini_rag_flow

# =================================================================
# SERVICE PORTS AND HOSTS
# =================================================================

# LiteLLM Proxy
LITELLM_HOST=0.0.0.0
LITELLM_PORT=4000
LITELLM_API_KEY=your_litellm_api_key_here

# vLLM Services
VLLM_QWEN25_HOST=0.0.0.0
VLLM_QWEN25_PORT=9998
VLLM_QWEN3_HOST=0.0.0.0
VLLM_QWEN3_PORT=9999

# Xinference
XINFERENCE_HOST=0.0.0.0
XINFERENCE_PORT=9900

# RAGFlow
RAGFLOW_HOST=0.0.0.0
RAGFLOW_PORT=9380
RAGFLOW_IMAGE=infiniflow/ragflow:v0.20.4-slim

# =================================================================
# GPU AND HARDWARE CONFIGURATION
# =================================================================

# GPU Device IDs (comma-separated for multiple GPUs)
CUDA_VISIBLE_DEVICES=0,1
NVIDIA_VISIBLE_DEVICES=all

# GPU Memory Utilization (0.0-1.0)
VLLM_GPU_MEMORY_UTILIZATION=0.95

# =================================================================
# MODEL CONFIGURATION
# =================================================================

# vLLM Model Settings
VLLM_MODEL_QWEN25=Qwen/Qwen2.5-14B-Instruct-1M
VLLM_MODEL_QWEN3=Qwen/Qwen3-30B-A3B-Instruct-2507
VLLM_TENSOR_PARALLEL_SIZE=1
VLLM_MAX_MODEL_LEN_QWEN25=329152
VLLM_MAX_MODEL_LEN_QWEN3=262144
VLLM_TRUST_REMOTE_CODE=true
VLLM_ENABLE_AUTO_TOOL_CHOICE=true
VLLM_TOOL_CALL_PARSER=hermes

# Xinference Model Settings
XINFERENCE_MODEL_SRC=huggingface
TORCH_USE_CUDA_DSA=1
PYTORCH_DISABLE_CUDA_ASSERTS=1

# =================================================================
# MONITORING AND LOGGING
# =================================================================

# Prometheus
PROMETHEUS_RETENTION_TIME=15d
PROMETHEUS_PORT=9090

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json

# =================================================================
# SECURITY AND AUTHENTICATION
# =================================================================

# JWT and API Keys
JWT_SECRET_KEY=your_jwt_secret_key_here
API_KEY_SECRET=your_api_key_secret_here

# SSL/TLS (for production)
SSL_CERT_PATH=/path/to/ssl/cert.pem
SSL_KEY_PATH=/path/to/ssl/private.key

# =================================================================
# NETWORKING AND PROXY
# =================================================================

# Docker Network
DOCKER_NETWORK=shared-vllm-network

# Reverse Proxy (if using nginx/caddy)
PROXY_HOST=localhost
PROXY_PORT=80
PROXY_SSL_PORT=443

# =================================================================
# RESOURCE LIMITS AND SCALING
# =================================================================

# Memory Limits
MEM_LIMIT=8073741824  # 8GB in bytes

# CPU Limits
CPU_LIMIT=4.0

# Disk Space
DISK_QUOTA=100GB

# =================================================================
# BACKUP AND RECOVERY
# =================================================================

# Backup Settings
BACKUP_RETENTION_DAYS=30
BACKUP_SCHEDULE=daily
BACKUP_PATH=/opt/backups

# =================================================================
# DEVELOPMENT AND DEBUGGING
# =================================================================

# Debug Mode
DEBUG=false
DEVELOPMENT_MODE=false

# Profiling
ENABLE_PROFILING=false
PROFILING_OUTPUT_PATH=/tmp/profiles

# =================================================================
# EXTERNAL INTEGRATIONS
# =================================================================

# OpenAI API (for fallback or hybrid deployment)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1

# Anthropic Claude (alternative provider)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Google Vertex AI
GOOGLE_APPLICATION_CREDENTIALS=/path/to/google/credentials.json

# =================================================================
# TIMEZONE AND LOCALIZATION
# =================================================================

# System Timezone
TIMEZONE=Asia/Shanghai

# Locale
LOCALE=en_US.UTF-8

# =================================================================
# THIRD-PARTY SERVICE INTEGRATIONS
# =================================================================

# Sentry (error tracking)
SENTRY_DSN=your_sentry_dsn_here

# DataDog (monitoring)
DATADOG_API_KEY=your_datadog_api_key_here

# Slack (notifications)
SLACK_WEBHOOK_URL=your_slack_webhook_url_here

# =================================================================
# ADVANCED CONFIGURATION
# =================================================================

# Docker Compose Project Name
COMPOSE_PROJECT_NAME=vllm-deployment

# Docker BuildKit
DOCKER_BUILDKIT=1

# Docker Registry (for private images)
DOCKER_REGISTRY=your-registry.com
DOCKER_REGISTRY_USER=your-registry-user
DOCKER_REGISTRY_PASSWORD=your-registry-password

# =================================================================
# ENVIRONMENT SPECIFIC OVERRIDES
# =================================================================

# Production overrides
# PROD_LITELLM_PORT=443
# PROD_RAGFLOW_PORT=443
# PROD_ENABLE_SSL=true

# Development overrides
# DEV_DEBUG=true
# DEV_LOG_LEVEL=DEBUG
# DEV_ENABLE_PROFILING=true

# =================================================================
# VALIDATION CHECKLIST
# =================================================================
#
# Before starting the deployment, ensure:
# ✅ HUGGING_FACE_HUB_TOKEN is set
# ✅ All database passwords are strong and unique
# ✅ GPU devices are properly configured
# ✅ Ports don't conflict with existing services
# ✅ File paths exist and are accessible
# ✅ API keys are valid and have proper permissions
# ✅ SSL certificates are valid (for production)
#
# Test your configuration:
# make env-check
# =================================================================
